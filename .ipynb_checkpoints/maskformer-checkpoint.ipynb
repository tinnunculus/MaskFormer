{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65aab802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from einops import rearrange\n",
    "from swin import Swin_transformer\n",
    "from transformer import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a31f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(on) {\n",
       "const e=$( \"<a>Setup failed</a>\" );\n",
       "const ns=\"js_jupyter_suppress_warnings\";\n",
       "var cssrules=$(\"#\"+ns);\n",
       "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
       "e.click(function() {\n",
       "    var s='Showing';  \n",
       "    cssrules.empty()\n",
       "    if(on) {\n",
       "        s='Hiding';\n",
       "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
       "    }\n",
       "    e.text(s+' warnings (click to toggle)');\n",
       "    on=!on;\n",
       "}).click();\n",
       "$(element).append(e);\n",
       "})(true);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40ec78",
   "metadata": {},
   "source": [
    "## Pixel Decoder\n",
    "* FPN 모델 구조를 따왔음.\n",
    "* FPN 모델 구조를 따왔지만 layer 마다 output을 뽑아서 학습을 하지는 않도록 했다.\n",
    "* Swin transformer의 layer에서 feature map을 따올 때, 이미지 형태의 구조(b, c, h, w)로 따와야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca384ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pixel_decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: list = [96 * 8, 96 * 4, 96 * 2, 96], \n",
    "        channels: int = 256,\n",
    "        n_groups: int = 16\n",
    "    ):\n",
    "        super(Pixel_decoder, self).__init__()\n",
    "        self.num_stage = len(in_channels)\n",
    "        self.from_encoder_projection_list = nn.ModuleList([])\n",
    "        self.from_feature_projection_list = nn.ModuleList([])\n",
    "        \n",
    "        # 첫번째 layer는 encoder에서 들어오는 feature에 대한 projection이 필요가 없음.\n",
    "        for i, in_channel in enumerate(in_channels):\n",
    "            if i == 0:\n",
    "                from_feature_projection = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.GroupNorm(n_groups, channels),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "                \n",
    "                self.from_encoder_projection_list.append(None)\n",
    "                self.from_feature_projection_list.append(from_feature_projection)\n",
    "            else:\n",
    "                from_feature_projection = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.GroupNorm(n_groups, channels),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "                from_encoder_projection = nn.Sequential(\n",
    "                    nn.Conv2d(in_channel, channels, kernel_size=1, stride=1),\n",
    "                    nn.GroupNorm(n_groups, channels)\n",
    "                )\n",
    "                \n",
    "                self.from_encoder_projection_list.append(from_encoder_projection)\n",
    "                self.from_feature_projection_list.append(from_feature_projection)\n",
    "        \n",
    "        self.final_projection = nn.Conv2d(channels, channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        '''\n",
    "            features : dict keys : stage1, stage2, stage3, stage4\n",
    "        '''\n",
    "        feature = self.from_feature_projection_list[0](features['stage4'])\n",
    "        \n",
    "        for i, (encoder_projection, feature_projection) in enumerate(zip(self.from_encoder_projection_list[1:], self.from_feature_projection_list[1:])):\n",
    "            feature = encoder_projection(features['stage' + str(3-i)]) + F.interpolate(feature, scale_factor=2, mode=\"nearest\")\n",
    "            feature = feature_projection(feature)\n",
    "        \n",
    "        return self.final_projection(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e3309",
   "metadata": {},
   "source": [
    "## Transformer decoder\n",
    "* DETR의 Transformer decoder와 동일하다.\n",
    "* positional embedding을 DETR에서 사용한 방법이 아닌 Swin에서 사용한 방법을 쓰고 싶었지만 Window 기반이 아니기에 Table 크기가 너무 커져 비효율적인듯...?\n",
    "* 모델에 저장되어 있는 쿼리는 배치가 1이기 때문에 이미지의 배치에 맞게 복사를 해야한다.\n",
    "* 우선 auxiary 학습을 위해 Transformer의 모든 layer마다 출력을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f04c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_decoder(Decoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_query = 100,\n",
    "        h = 8, \n",
    "        d_model = 256, \n",
    "        d_ff = 512, \n",
    "        dropout = 0.1, \n",
    "        N = 10,\n",
    "        in_channels = 768,\n",
    "        feature_size = (16, 16)\n",
    "    ):\n",
    "        super().__init__(h = h, d_model = d_model, d_ff = d_ff, dropout = dropout, N = N)\n",
    "        self.n_query = n_query\n",
    "        self.queries = nn.Parameter(torch.rand(1, n_query, d_model))\n",
    "        self.linear = nn.Linear(in_channels, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.rand(1, in_channels, feature_size[0], feature_size[1]))\n",
    "\n",
    "    def forward(self, feature):\n",
    "        b, c, h, w = feature.shape\n",
    "        positional_encoding = F.interpolate(self.positional_encoding, (h, w), mode=\"bilinear\")\n",
    "        keyvalue = rearrange(feature, 'b c h w -> b (h w) c') + rearrange(positional_encoding, 'b c h w -> b (h w) c')\n",
    "        keyvalue = self.linear(keyvalue)\n",
    "        queries = self.queries.expand(b, self.n_query, -1)\n",
    "        return super().forward(queries, keyvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad7e13",
   "metadata": {},
   "source": [
    "## Segmentation Module\n",
    "* Transformer decoder의 output을 가지고 segmentation vector와 classification vector를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c29a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation_module(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_class,\n",
    "        in_channels = 256,\n",
    "        inter_channels = 256,\n",
    "        out_channels = 256,\n",
    "    ):\n",
    "        super(Segmentation_module, self).__init__()\n",
    "        self.mlp_segmentation_mask = nn.Sequential(\n",
    "            nn.Linear(in_channels, inter_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inter_channels, out_channels),\n",
    "        )\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(in_channels, n_class + 1),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, mask_embedded_vec):\n",
    "        return self.mlp_segmentation_mask(mask_embedded_vec), self.classification(mask_embedded_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b192fc3",
   "metadata": {},
   "source": [
    "## MaskFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5deef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskFormer(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(MaskFormer, self).__init__()\n",
    "        self.backbone = Swin_transformer(\n",
    "            patch_size = model_config[\"backbone_patch_size\"], \n",
    "            window_size = model_config[\"backbone_window_size\"], \n",
    "            merge_size = model_config[\"backbone_merge_size\"], \n",
    "            model_dim = model_config[\"backbone_model_dim\"], \n",
    "            num_layers_in_stage = model_config[\"backbone_num_layers_in_stage\"]\n",
    "        )\n",
    "        \n",
    "        in_channels = list(model_config[\"backbone_model_dim\"] * 2 ** i for i in range(len(model_config[\"backbone_num_layers_in_stage\"])))[::-1]\n",
    "        self.pixel_decoder = Pixel_decoder(\n",
    "            in_channels = in_channels,\n",
    "            channels = model_config[\"pixel_decoder_channels\"],\n",
    "            n_groups = model_config[\"pixel_decoder_n_groups\"]\n",
    "        )\n",
    "        \n",
    "        in_channels = model_config[\"backbone_model_dim\"] * 2 ** (len(model_config[\"backbone_num_layers_in_stage\"]) - 1)\n",
    "        self.transformer_decoder = Transformer_decoder(\n",
    "            n_query = model_config[\"transformer_decoder_num_query\"],\n",
    "            h = model_config[\"transformer_decoder_num_head\"], \n",
    "            d_model = model_config[\"transformer_decoder_dimension\"],\n",
    "            dropout = model_config[\"transformer_decoder_dropout\"],\n",
    "            N = model_config[\"transformer_decoder_num_layer\"],\n",
    "            in_channels = in_channels,\n",
    "            feature_size = model_config[\"transformer_decoder_positional_size\"]\n",
    "        )\n",
    "        \n",
    "        self.segmentation_module = Segmentation_module(\n",
    "            n_class = model_config[\"segmentation_module_num_class\"], \n",
    "            in_channels = model_config[\"segmentation_module_in_channels\"],\n",
    "            out_channels = model_config[\"segmentation_module_out_channels\"]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        pixel_feature = self.pixel_decoder(features)\n",
    "        b, C, H, W = pixel_feature.shape\n",
    "        \n",
    "        mask_embedded_vec = self.transformer_decoder(features['stage4'])[-1]\n",
    "        \n",
    "        segmentation_mask_vecs, classification_vecs = self.segmentation_module(mask_embedded_vec)\n",
    "        \n",
    "        segmentation_mask = torch.matmul(segmentation_mask_vecs, pixel_feature.view(b, C, -1)).view(b, -1, H, W)\n",
    "        segmentation_mask = F.sigmoid(segmentation_mask)\n",
    "        \n",
    "        result = {}\n",
    "        result[\"pred_masks\"] = segmentation_mask\n",
    "        result[\"pred_logits\"] = classification_vecs\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9925f",
   "metadata": {},
   "source": [
    "## matching\n",
    "* DETR과 동일하게 bipartite maching을 한다.\n",
    "* mask loss는 focal loss와 dice loss를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33701bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianMatcher(nn.Module):\n",
    "    def __init__(self, w_class: float = 1, w_focal: float = 1, w_dice: float = 1):\n",
    "        super().__init__()\n",
    "        self.w_class = w_class\n",
    "        self.w_focal = w_focal\n",
    "        self.w_dice = w_dice\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def dice_cost(self, predict, target):\n",
    "        # predict : b * n_queries, h * w\n",
    "        # target : b * n_obj, h * w\n",
    "        numerator = 2 * (predict[:, None, :] * target[None, :, :]).sum(-1)\n",
    "        denominator = predict.sum(-1)[:, None] + target.sum(-1)[None, :]\n",
    "        cost_dice = 1 - (numerator + 1) / (denominator + 1)\n",
    "        return cost_dice\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def focal_cost(self, predict, target, gamma = 2., alpha = 0.25):\n",
    "        # predict : b * n_queries, h * w\n",
    "        # target : b * n_obj, h * w\n",
    "        predict = predict[:, None, :].expand((predict.shape[0], target.shape[0], predict.shape[1]))\n",
    "        target = target[None, :, :].expand((predict.shape[0], target.shape[0], target.shape[1]))\n",
    "        ce = F.binary_cross_entropy_with_logits(predict, target, reduction='none')\n",
    "\n",
    "        p_t = predict * target + (1 - predict) * (1 - target)\n",
    "        focal_cost = ce * ((1 - p_t) ** gamma)\n",
    "\n",
    "        alpha_t = alpha * target + (1 - alpha) * (1 - target)\n",
    "        focal_cost = alpha_t * focal_cost\n",
    "        return focal_cost.mean(-1)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, out, targets):\n",
    "        pred_logits = out[\"pred_logits\"] # b, n, class + 1\n",
    "        pred_masks = out[\"pred_masks\"] # b, n, h, w\n",
    "        target_logits = targets[\"labels\"] # [ m_i for i in b]\n",
    "        target_masks = targets[\"masks\"] # [ m_i, h, w for i in b]\n",
    "        bs, num_queries = pred_logits.shape[:2]\n",
    "        device = pred_logits.device\n",
    "        \n",
    "        out_prob = pred_logits.flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_mask = pred_masks.flatten(0, 1).flatten(1, 2)  # [batch_size * num_queries, h * w]\n",
    "\n",
    "        tgt_ids = torch.cat([v for v in target_logits]) # [batch_size * num_obj]\n",
    "        tgt_mask = torch.cat([v for v in target_masks]).flatten(1, 2) # [batch_size * num_obj, h * w]\n",
    "\n",
    "        # cost :\n",
    "        #     row : pred_querys\n",
    "        #     col : target_obj\n",
    "        cost_class = -out_prob[:, tgt_ids]                                                          # [batch_size * num_queries, batch_size * num_obj]\n",
    "        cost_dice = self.dice_cost(out_mask, tgt_mask)                                              # [batch_size * num_queries, batch_size * num_obj] \n",
    "        cost_focal = self.focal_cost(out_mask, tgt_mask)                                            # [batch_size * num_queries, batch_size * num_obj]\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.w_dice * cost_dice + self.w_class * cost_class + self.w_focal * cost_focal\n",
    "        C = C.view(bs, num_queries, -1).cpu() # [batch_size, num_queries, batch_size * num_obj]\n",
    "\n",
    "        sizes = [len(v) for v in target_masks]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        result = []\n",
    "        for i, j in indices:\n",
    "            i = torch.as_tensor(i, dtype=torch.int64, device=device)\n",
    "            j = torch.as_tensor(j, dtype=torch.int64, device=device)\n",
    "            result.append(i[j])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d024cf",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5853a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maskformer_loss(nn.Module):\n",
    "    def __init__(self, w_focal: float = 1., w_dice: float = 1., w_class: float = 1., w_noobj: float = 1.):\n",
    "        super(Maskformer_loss, self).__init__()\n",
    "        self.w_class = w_class\n",
    "        self.w_focal = w_focal\n",
    "        self.w_dice = w_dice\n",
    "        self.w_noobj = w_noobj\n",
    "        \n",
    "    def class_loss(self, pred_logits, target_logits, match_indexs):\n",
    "        device = pred_logits.device\n",
    "        target_labels = torch.zeros(pred_logits.shape[:2], dtype=torch.int64, device=device)\n",
    "        cost_no_obj = torch.ones(pred_logits.shape[2], device=device)\n",
    "        cost_no_obj[0] *= self.w_noobj\n",
    "        \n",
    "        for i, match_index in enumerate(match_indexs):\n",
    "            target_labels[i, match_index] = target_logits[i]\n",
    "        \n",
    "        class_loss = F.cross_entropy(pred_logits.flatten(0, 1), target_labels.flatten(0, 1), cost_no_obj)\n",
    "        return class_loss\n",
    "        \n",
    "    def focal_loss(self, predict, target, gamma = 2.0, alpha = 0.25):\n",
    "        # predict : b * n_queries, h * w\n",
    "        # target : b * n_obj, h * w\n",
    "        ce = F.binary_cross_entropy_with_logits(predict, target, reduction='none')\n",
    "\n",
    "        p_t = predict * target + (1 - predict) * (1 - target)\n",
    "        focal_cost = ce * ((1 - p_t) ** gamma)\n",
    "\n",
    "        alpha_t = alpha * target + (1 - alpha) * (1 - target)\n",
    "        focal_loss = alpha_t * focal_cost\n",
    "        return focal_loss.mean()\n",
    "        \n",
    "    def dice_loss(self, predict, target):\n",
    "        numerator = 2 * (predict * target).sum(-1)\n",
    "        denominator = predict.sum(-1) + target.sum(-1)\n",
    "        loss_dice = 1 - (numerator + 1) / (denominator + 1)\n",
    "        return loss_dice.mean()\n",
    "        \n",
    "    def forward(self, out, targets, match_indexs):\n",
    "        pred_logits = out[\"pred_logits\"] # b, n, class + 1\n",
    "        pred_boxes = out[\"pred_masks\"] # b, n, h, w\n",
    "        target_logits = targets[\"labels\"] # [ m_i for i in b]\n",
    "        target_boxes = targets[\"masks\"] # [ m_i, h, w for i in b]\n",
    "        \n",
    "        tgt_mask = torch.cat([v for v in target_boxes]).flatten(1, 2) # [batch_size * num_obj, h * w]\n",
    "        out_mask = pred_boxes.flatten(2)  # [batch_size, num_queries, h * w]\n",
    "        out_mask = torch.cat([out_mask[i, match_index, :] for i, match_index in enumerate(match_indexs)]) # [batch_size * num_obj, h * w]\n",
    "        \n",
    "        class_loss = self.class_loss(pred_logits, target_logits, match_indexs) * self.w_class\n",
    "        focal_loss = self.focal_loss(out_mask, tgt_mask) * self.w_focal\n",
    "        dice_loss = self.dice_loss(out_mask, tgt_mask) * self.w_dice\n",
    "        \n",
    "        return class_loss + focal_loss + dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9528aa",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac9ffae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5244541",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"backbone_patch_size\" : 4,\n",
    "    \"backbone_window_size\" : 8,\n",
    "    \"backbone_merge_size\" : 2,\n",
    "    \"backbone_model_dim\" : 96,\n",
    "    \"backbone_num_layers_in_stage\" : [2, 2, 6, 2],\n",
    "    \"pixel_decoder_n_groups\" : 16,\n",
    "    \"pixel_decoder_channels\" : 256,\n",
    "    \"transformer_decoder_positional_size\" : (16, 16),\n",
    "    \"transformer_decoder_num_query\" : 100,\n",
    "    \"transformer_decoder_dimension\" : 256,\n",
    "    \"transformer_decoder_num_head\" : 8,\n",
    "    \"transformer_decoder_dropout\" : 0,\n",
    "    \"transformer_decoder_num_layer\" : 6,\n",
    "    \"segmentation_module_in_channels\" : 256,\n",
    "    \"segmentation_module_num_class\" : 10,\n",
    "    \"segmentation_module_out_channels\" : 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddc2d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swin transformer 때문에 아직 256의 배수 크기 밖에 안된다. 이거 나중에 고치자\n",
    "img = torch.randn((2, 3, 512, 512)).cuda()\n",
    "target = {}\n",
    "target['labels'] = [torch.zeros((15), dtype=torch.long).cuda(), torch.ones((4), dtype=torch.long).cuda()]\n",
    "target['masks'] = [torch.zeros((15, 128, 128)).cuda(), torch.ones((4, 128, 128)).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b9ba11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskformer = MaskFormer(model_config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe99cf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/home/lee0301jy/envs/torch_vir/lib/python3.6/site-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/data1/home/lee0301jy/envs/torch_vir/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "result = maskformer(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23da450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pred_masks', 'pred_logits'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56f71aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 128, 128]) torch.Size([2, 100, 11])\n"
     ]
    }
   ],
   "source": [
    "print(result[\"pred_masks\"].shape, result[\"pred_logits\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d812f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher().cuda()\n",
    "Loss = Maskformer_loss(w_noobj=0.1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "419a497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3066, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "match_indexs = matcher(result, target)\n",
    "loss = Loss(result, target, match_indexs)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
